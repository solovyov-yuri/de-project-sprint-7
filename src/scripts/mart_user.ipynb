{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from logging import Logger\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from timezonefinder import TimezoneFinder\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV Constants\n",
    "GEO_DIR = \"/user/solovyovyu/geo.csv\"\n",
    "# EVENTS_DIR = \"/user/solovyovyu/data/geo/events\"\n",
    "OUT_PATH = \"/user/solovyovyu/analytics\"\n",
    "\n",
    "# PROD Constants\n",
    "EVENTS_DIR = \"/user/master/data/geo/events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure logging\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(message)s\")\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timezone(lat, lng):\n",
    "    if lat is None or lng is None:\n",
    "        print(\"lat or lng is not defined\")\n",
    "        return None\n",
    "    tf = TimezoneFinder()\n",
    "    return tf.timezone_at(lat=lat, lng=lng)\n",
    "\n",
    "get_timezone_udf = F.udf(get_timezone, StringType())\n",
    "\n",
    "def read_events(event_type: str, events_dir: str, spark: SparkSession, logger: Logger) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read events from parquet file and filter by event_type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.parquet(events_dir) \\\n",
    "            .where(F.col(\"event_type\") == event_type) \\\n",
    "            .select(\n",
    "                F.col(\"event.message_id\"),\n",
    "                F.coalesce(\n",
    "                    F.col(\"event.message_from\"),\n",
    "                    F.col(\"event.reaction_from\"),\n",
    "                    F.col(\"event.user\")\n",
    "                ).alias(\"user_id\"),\n",
    "                F.coalesce(F.col(\"event.message_ts\"), F.col(\"event.datetime\")).alias(\"datetime\"),\n",
    "                \"lat\",\n",
    "                \"lon\"\n",
    "            )\n",
    "        logger.info(f\"Events {event_type} are read from {events_dir}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while reading events: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_geo(geo_dir: str, spark: SparkSession, logger: Logger) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read geo data from csv file. Add timezone column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.options(delimiter=\";\", header=True).csv(geo_dir) \\\n",
    "            .withColumn(\"lat\", F.regexp_replace(\"lat\", \",\", \".\").cast(DoubleType())) \\\n",
    "            .withColumn(\"lng\", F.regexp_replace(\"lng\", \",\", \".\").cast(DoubleType())) \\\n",
    "            .withColumn(\"timezone\", get_timezone_udf(F.col(\"lat\"), F.col(\"lng\"))) \\\n",
    "            .withColumnRenamed(\"lat\", \"geo_lat\") \\\n",
    "            .withColumnRenamed(\"lng\", \"geo_lon\")\n",
    "\n",
    "        logger.info(f\"Geo data is read from {geo_dir}.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while reading geo data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Радиус Земли в километрах\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def haversine_udf(lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series) -> pd.Series:\n",
    "    return haversine(lat1, lon1, lat2, lon2)\n",
    "\n",
    "\n",
    "def add_distance(events_df: DataFrame, geo_df: DataFrame, logger: Logger) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add distants to each city from geo_df.\n",
    "    \"\"\"\n",
    "    geo_df = F.broadcast(geo_df)\n",
    "\n",
    "    result_df = events_df.join(geo_df, how=\"cross\") \\\n",
    "        .withColumn(\n",
    "            \"distance\",\n",
    "            haversine_udf(\n",
    "                F.col(\"lat\"), F.col(\"lon\"), F.col(\"geo_lat\"), F.col(\"geo_lon\")\n",
    "            )\n",
    "        ) \\\n",
    "        .select(\n",
    "            \"user_id\",\n",
    "            \"message_id\",\n",
    "            \"datetime\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"geo_lat\",\n",
    "            \"geo_lon\",\n",
    "            \"distance\",\n",
    "            \"city\",\n",
    "            \"timezone\"\n",
    "        )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def add_closest_city(df: DataFrame, logger: Logger) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add city that close to event.\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(\"message_id\", \"user_id\").orderBy(\"distance\")\n",
    "\n",
    "    result_df = df \\\n",
    "        .withColumn(\"rank\", F.row_number().over(window))\\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\n",
    "            \"message_id\",\n",
    "            \"user_id\",\n",
    "            \"datetime\",\n",
    "            \"city\",\n",
    "            \"timezone\"\n",
    "        )\n",
    "\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Mart User\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:05:32,657 - __main__ - Geo data is read from /user/solovyovyu/geo.csv.\n"
     ]
    }
   ],
   "source": [
    "geo_df = read_geo(GEO_DIR, spark, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 16:05:37 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "2025-02-12 16:05:37,971 - __main__ - Events message are read from /user/master/data/geo/events.\n"
     ]
    }
   ],
   "source": [
    "message_df = read_events(\"message\", EVENTS_DIR, spark, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_with_distance = add_distance(message_df, geo_df, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_with_city = add_closest_city(messages_with_distance, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"user_id\", \"city\").orderBy(\"datetime\")\n",
    "\n",
    "home_city_df = messages_with_city \\\n",
    "    .withColumn(\"prev_city\", F.lag(\"city\").over(window)) \\\n",
    "    .withColumn(\"is_new_city\", (F.col(\"city\") != F.col(\"prev_city\")).cast(\"int\")) \\\n",
    "    .withColumn(\"group_id\", F.sum(\"is_new_city\").over(window)) \\\n",
    "    .withColumn(\"stay_duration\", F.datediff(F.lead(\"datetime\").over(window), F.col(\"datetime\"))) \\\n",
    "    .filter(F.col(\"stay_duration\") >= 27) \\\n",
    "    .groupBy(\"user_id\", \"city\").agg(\n",
    "        F.max(\"stay_duration\").alias(\"total_stay\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_stay\", ascending=False) \\\n",
    "    .groupBy(\"user_id\").agg(\n",
    "        F.first(\"city\").alias(\"home_city\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mart_user = messages_with_city \\\n",
    "    .groupBy(\"user_id\").agg(\n",
    "        F.last(\"city\").alias(\"act_city\")\n",
    "    ).join(home_city_df, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_features = messages_with_city.groupBy(\"user_id\").agg(\n",
    "    F.collect_list(\"city\").alias(\"travel_array\"),\n",
    "    F.count(\"city\").alias(\"travel_count\")\n",
    ")\n",
    "mart_user = mart_user.join(travel_features, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"user_id\").orderBy(\"datetime\")\n",
    "\n",
    "user_last_message_local_time = messages_with_city \\\n",
    "    .withColumn(\"rank\", F.rank().over(window)) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .withColumn(\"local_time\", F.from_utc_timestamp(F.col(\"datetime\"), F.col(\"timezone\"))) \\\n",
    "    .select(\"user_id\", \"local_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mart_user = mart_user.join(user_last_message_local_time, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:06:02,852 - root - KeyboardInterrupt while sending command.1468]8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "2025-02-12 16:06:02,853 - py4j.clientserver - Closing down clientserver connection\n",
      "25/02/12 16:06:02 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 446)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_36708/2258226582.py\", line 5, in get_timezone\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/timezonefinder.py\", line 325, in __init__\n",
      "    super().__init__(bin_file_location, in_memory)\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/timezonefinder.py\", line 92, in __init__\n",
      "    self.shortcut_mapping = read_shortcuts_binary(path2shortcut_bin)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/hex_helpers.py\", line 58, in read_shortcuts_binary\n",
      "    poly_ids: np.ndarray = np.fromfile(\n",
      "                           ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/12 16:06:02 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 446) (172.20.57.228 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_36708/2258226582.py\", line 5, in get_timezone\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/timezonefinder.py\", line 325, in __init__\n",
      "    super().__init__(bin_file_location, in_memory)\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/timezonefinder.py\", line 92, in __init__\n",
      "    self.shortcut_mapping = read_shortcuts_binary(path2shortcut_bin)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/samogonn/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/timezonefinder/hex_helpers.py\", line 58, in read_shortcuts_binary\n",
      "    poly_ids: np.ndarray = np.fromfile(\n",
      "                           ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/02/12 16:06:02 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job\n",
      "25/02/12 16:06:02 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 447) (172.20.57.228 executor driver): TaskKilled (Stage cancelled: Job 19 cancelled )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmart_user\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/mart_user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/projects/de-project-sprint-7/.venv/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mart_user.write.mode(\"overwrite\").partitionBy().format(\"parquet\").save(f\"{OUT_PATH}/mart_user\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
